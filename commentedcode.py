# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dr5Ss4KDc5aa_fGgOZ8yE9ZhqzbS93Fr
"""

# Importing libraries that are required
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report

# Loading the Credit Risk Prediction dataset
data = pd.read_csv("Credit Risk Prediction.csv")

#Rename the target column by reducing it into simple for usage
data.rename(columns={'Will_Default_Next_Month': 'Target'}, inplace=True)

#Drop Client_ID which is nothing useful but identification
data.drop(['Client_ID'], axis=1, inplace=True)

#Filling missing values with 0 using fillna()
data.fillna(0, inplace=True)

# Creating new column Total_Delays for counting in one unit how many times payment was delayed
data['Total_Delays'] = data[[f'Repay_{m}' for m in ['Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']]].gt(0).sum(axis=1)

# Creating new column Average payment ratio that equals total paid divides total billed
data['Avg_Payment_Ratio'] = data[[f'PaidAmt_{m}' for m in ['Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']]].sum(axis=1) / \
                            data[[f'BillAmt_{m}' for m in ['Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']]].sum(axis=1).replace(0, 1)

#Creating new column Payment consistency that equals std deviation of payment amount
data['Payment_Consistency'] = data[[f'PaidAmt_{m}' for m in ['Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']]].std(axis=1)

#Creating new column Credit utilization that equals avg bill amount divides credit limit
data['Credit_Utilization'] = data[[f'BillAmt_{m}' for m in ['Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']]].mean(axis=1) / \
                             data['Credit_Limit'].replace(0, 1)

#Splitting data into X features and y target
X = data.drop('Target', axis=1)
y = data['Target']

#Splitting data into train 80 and test 20 data sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

#Identifying the categorical and numerical columns
cat_cols = ['Gender_Code', 'Education_Level', 'Marital_Status']
num_cols = [col for col in X.columns if col not in cat_cols]

#Preprocessing data Scaling numerical columns and encoding categorical columns
preprocess = ColumnTransformer([
    ('scale', StandardScaler(), num_cols),
    ('encode', OneHotEncoder(drop='first'), cat_cols)
])

#defining the models that we are going to train and compare
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(probability=True)
}

#importing library for visualisation
import matplotlib.pyplot as plt
#importing library for model evaluation
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay
#Training and evaluating each model
for model, object in models.items():
    print(f"\n{model}")

    # building pipeline for preprocessing and training the model in one clean step
    pipeline = Pipeline([
        ('prep', preprocess),
        ('model', object)
    ])
    pipeline.fit(X_train, y_train)

    #making Predictions
    y_pred = pipeline.predict(X_test)
    y_proba = pipeline.predict_proba(X_test)[:, 1]

    # Evaluation Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)

    # Printing Evaluation Report
    print(f"Accuracy     : {acc:.4f}")
    print(f"Precision    : {prec:.4f}")
    print(f"Recall       : {rec:.4f}")
    print(f"F1 Score     : {f1:.4f}")
    print(f"ROC AUC      : {roc_auc:.4f}")
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()
    plt.title(f"{model} - Confusion Matrix")
    plt.show()

    # Visualising ROC Curve
    RocCurveDisplay.from_predictions(y_test, y_proba)
    plt.title(f"{model} - ROC Curve")
    plt.show()